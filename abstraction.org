* Motivation
This is an attempt to better understand how the process of abstraction can be used in computational models of intelligence.  It is also an experiment in literate-style programming and questions related to literate programming will be prefaced with LP?
* Approach
One way to formulate the problem of modeling intelligence is by vieweing it as a process of inference over incomplete data influenced by previously perceived data.  As a starting point we can imagine data as being a (possibly nested) list of symbols.  An inference problem can be described as a (possibly nested) list containing variables where the variables represent an unknown part of the list that should be filled in.
* Examples
** Binary Classification
An example of a problem that can be formulated in terms of inference on lists is classification of binary data.  We can write input data as
#+name: binary_data
#+begin_src python :results value
pattern0 = [[[1], 0], [[1,1], 0], [[1,1,1,1,1], 0], [[1,1], 0]]
pattern1 = [[[1,1,0,0],1], [[1,1,1,0,0,0],1], [[1,1,1,1,0,0,0,0],1]]
#+end_src

With org-mode's literate programming support we can display this data in a nice format like so
#+begin_src python :results value :noweb yes
  <<binary_data>>
  return pattern0+pattern1
#+end_src

#+RESULTS:
| (1)               | 0 |
| (1 1)             | 0 |
| (1 1 1 1 1)       | 0 |
| (1 1)             | 0 |
| (1 1 0 0)         | 1 |
| (1 1 1 0 0 0)     | 1 |
| (1 1 1 1 0 0 0 0) | 1 |

*** First Approximation
A simple (but expected to be poor performing) algorithm for the inference problem would be to store every piece of data then when an inference problem (i.e. partial piece of data) is faced we try to match what is known to any of the pieces of data that have already been seen and then use a matching piece of data to fill in the unknowns in the inference problem.  

Even though it's not hard to think of cases where this algorithm won't work, we start with this problem to build up some of the general infrastructure for modeling inference and abstraction and make the idea of modeling intelligence as inference on lists more concrete.

**** TODO Storing data
The first part of the algorithm is to store data.  We'll use a list as the data structure to keep things simple for now, but use a layer of abstraction to manipulate it.
#+name: memory_deprecated
#+begin_src python :noweb yes :tangle no
  class Memory:
      memory = []
      def add(self, data):
          self.memory.append(data)
          
#+end_src

Now we test the function using the binary data from earlier
#+name: add_pattern0_to_memory
#+begin_src python :noweb yes
  <<binary_data>>
  <<memory>>
  memory = Memory()
  for data in pattern0:
      memory.add(data)
  

  
#+end_src

#+name: return_memory
#+begin_src python :noweb yes
  <<add_pattern0_to_memory>>
  
  return memory.memory
#+end_src

#+RESULTS: return_memory
| (1)         | 0 |
| (1 1)       | 0 |
| (1 1 1 1 1) | 0 |
| (1 1)       | 0 |


***** LP? Python mode in src blocks?
What is the best way to edit code in the source blocks?  By default, emacs seems to be in org-mode.
****** C-c '
http://orgmode.org/manual/Editing-source-code.html#Editing-source-code

**** Inference
Now that we have a way to hold prior data we want to be able to use that data to solve inference problems.  Let's formalize our earlier approach of inference on nested lists.
***** Inference Formulation
An inference problem can be described as a list with some variables that represent unknown parts of the list.  Elements of the list can be either lists, symbols, or variables.  In code the class might look like

#+name: inference_problem
#+begin_src python :tangle yes
  class InferenceProblem:
      #variables that might appear in partial_data
      variables = []
      partial_data = []
  
      def __init__(self, initial_variables, initial_partial_data):
          self.variables = initial_variables
          self.partial_data = initial_partial_data
  
#+end_src

We could make the typing more explicit by performing checks during instantiation of the class, but we'll leave it unless this becomes an issue.

It's also worth noting that an inference problem is essentially the same thing as an abstraction.  Perhaps we'll use some kind of type aliasing if it becomes helpful when we explore abstraction-based approaches to inference.

****** Binary Data Inference Problems
******* Classification
One example of an inference problem is classification of binary data.  Suppose we have some data from either pattern0 or pattern1 above as shown earlier and we want to infer the pattern it came from, we can create a problem like so
#+name: binary_data_classification_problems_deprecated
#+begin_src python :results output :noweb yes :tangle no

  classify_problem0 = InferenceProblem(['v0'],[[1,1,1,1], 'v0'])  
  classify_problem1 = InferenceProblem(['v0'], [[1,1,1,0,0,0], 'v0'])

#+end_src


The solution for classify_problem0 being v0 is [0] and the solution for classify_problem1 being v0 is [1].  The reason the solutions are in a list is that variables may represent several missing elements in a list.  This is better illustrated in the next example.
******* Completion
Another type of problem would be to infer parts of the binary data given we know the pattern e.g.

#+name: binary_data_completion_problems
#+begin_src python :results output :noweb yes :tangle 
  complete_problem0 = InferenceProblem(['v0'], [[1,1,1,'v0'], 1])
  complete_problem1 = InferenceProblem(['v0', 'v1'], [['v0', 1, 'v1'], 0])
#+end_src
The solution for the first problem intuitively is v0 is [0, 0, 0].  The second problem, complete_problem1, raises the issue that problems do not necessarily have a unique answer.  For this problem v1 and v2 could be lists of 1 of arbitrary length e.g. v0 is [1, 1, 1] and v1 is [1] or v0 is [1,1] and v1 is [1,1,1,1].  We'll take a closer look at solutions to inference problems and evaluating them in the next section.

****** Binary Data Solutions
An inference problem is a set of variables along with a list containing those variables.  The solution to inference problems are values that are assigned to those variables (bindings or an environment) and are substituted in the following way.

#+name: substitute_function
#+begin_src python :results value :tangle yes
  def substitute(partial_data, bindings):
      """
      replace variables in the abstraction with the matching values in the bindings
      and return the resulting list
      """
      complete_list = []
      for element in partial_data:
          replacement = swap(element, bindings)
          complete_list += replacement
      return complete_list
  
  def swap(element, bindings):
          if element in bindings.keys():
              return bindings[element]
          elif isinstance(element, list):
              return [substitute(element, bindings)]
          else:
              return [element]
  
#+end_src

We can demonstrate the substitute function on our earlier examples of inference problems like so

#+name: substitution_examples
#+begin_src python :results output :noweb yes :tangle no
  <<substitute_function>>
  <<binary_data_classification_problems>>
  <<binary_data_completion_problems>>
  
  print substitute(classify_problem0.partial_data, {'v0': [0]})
  print substitute(classify_problem1.partial_data, {'v0': [1]})
  print substitute(complete_problem0.partial_data, {'v0': [0,0,0]})
  print substitute(complete_problem1.partial_data, {'v0': [1,1,1], 'v1': [1]})
  print substitute(complete_problem1.partial_data, {'v0': [1,1], 'v1': [1,1,1,1]})
#+end_src

#+RESULTS: substitution_examples
: ['v0']
: [[1, 1, 1, 1], 0]
: [[1, 1, 1, 0, 0, 0], 1]
: [[1, 1, 1, 0, 0, 0], 1]
: [[1, 1, 1, 1, 1], 0]
: [[1, 1, 1, 1, 1, 1, 1], 0]

#+RESULTS: subsitution_examples
Here we used the solutions, i.e. bindings, we thought were intuitively correct for each problem and we get completed lists that look like the prior data we had seen.

We assume for every inference problem there is some "true solution" or set of solutions for it and an algorithm for solving inference problems should try to produce the true solution for a given inference problem.

So for 
#+begin_src python
  classify_problem0 = InferenceProblem(['v0'],[[1,1,1,1], 'v0'])  
#+end_src 
the true solution could be defined as {'v0': [0]}.  
In the case of 
#+begin_src python
  complete_problem1 = InferenceProblem(['v0', 'v1'],[['v0',1,'v1'], 0])  
#+end_src 
the true solution might be the set of all lists containing 1s i.e. {[1], [1,1], [1,1,1],...}

How do we tell if one solution is better than the other?  So given
#+begin_src python
  complete_problem0 = InferenceProblem(['v0'],[[1,1,1,'v0'] '1'])  
  true_solution = {'v0', [0,0,0]}
#+end_src 
If we have an algorithm that has a choice between {'v0', [0,0,1]} and {'v0, [1,1,1]}, how does one compare the two solutions.  It's not clear, but this seems to be something that depends on the algorithm implementation rather than the problem specification.  In terms of the problem either the solution is incorrect or correct and in this case both solutions are incorrect since they don't match the true solution.

***** Algorithm for Inference
This section will describe more formally the simple algorithm for trying to solve inference problems introduced in the section "First Approximation."  Recall the idea is to create a solution, i.e. binding of variables, based on matching the partial data of the inference problem to previously seen data.  Another way to phrase it is we want to find an environment where the the partial data is the same as some prior data we've seen.

The process of matching seems like unification (http://en.wikipedia.org/wiki/Unification_(computer_science)) so it might be worth looking more into the literature if straight-forward approaches run into barriers.  For now though we'll develop our approach by looking at small examples and generalizing.  

Suppose our prior data consists of pattern0 i.e. we have executed the add_pattern0_to_memory code block and now we are faced with classify_problem0.  We want to try and match the partial data of classify_problem0 to the data we've already seen then use that to create a solution.
****** Matching
******* Implementation
Let's look at the pairing of partial data to previous data
#+name: iterate_match
#+begin_src python :results output :noweb yes :tangle no
  <<binary_data_classification_problems>>
  <<add_patter0_to_memory>>
  #TODO define an iterator for Memory to go through the saved data
  
  for memory in memory.memory:
      print classify_problem0.partial_data, memory
  
#+end_src

#+RESULTS: iterate_match
: [[1, 1, 1, 1], 'v0'] [[1], 0]
: [[1, 1, 1, 1], 'v0'] [[1, 1], 0]
: [[1, 1, 1, 1], 'v0'] [[1, 1, 1, 1, 1], 0]
: [[1, 1, 1, 1], 'v0'] [[1, 1], 0]

What should be the output of a matching e.g. if I try to match [[1, 1, 1, 1], 'v0'] with [[1], 0] what should the result be?  What about [[1, 1, 1, 1], 'v0'] and [[1, 1, 1, 1, 1], 0]?  In the future perhaps we'll develop a match function that returns {'v0': 0} as a binding for these two examples by making some sort of "best guess", but for a first pass let's assume there needs to be a perfect match of non-variable parts of the partial data for match to return any bindings (this may be enough when abstraction over prior data is added since matching against abstractions of prior data might have the same effect as making a best guess).  

We still must consider what to do when there are several possible valid matches.  An example being
#+name: example_data_many_matches
#+begin_src python  
  partial_data = [0, 'v0', 0, 'v1']
  prior_data = [0,1,0,1,0,1,0,1]
  
  valid_binding0 = {'v0': [1,0,1], 'v1': [1,0,1]}
  valid_binding1 = {'v0': [1], 'v1': [1,0,1,0,1]}
#+end_src

What is the return type for match? If there is no match then None should be returned.  Perhaps a single binding out of the possibly many bindings.  Maybe all the bindings.  In addition to the binding(s) it might also be useful to return some sort of score with the binding so that we may distinguish between the quality of different bindings.  What are the consequences of the different return types in the broader context of finding a solution to an inference problem?  

This raises another question worth asking about match is whether it should try to match to any part of the prior data or just the top level. E.g.
#name: example_sub_matching
#+begin_src python
  partial_data = [1,0,'v0']
  prior_data = [1,1,[1,0,1]]
  
  sub_match = {'v0', [1]}
#+end_src
Where the sub_match occurs from partial_data being matched to the third element of prior_data.  This is an interesting question because if this is the case then rather than viewing a solution to an inference problem as trying to match to each individual prior piece of data we can view the prior data as one large piece of data and solving the inference problem is a matter of finding a match of the partial_data to this single prior data object.  Either way we'll still probably want a function which matches only the top-level even if it's just a helper function.

In all cases it seems natural that the return of a matching should be a function of matching the individual elements of the partial data.  The possible types to be matched are primitives, variables, and lists.  

#+name: structure_for_match_deprecated
#+begin_src python :noweb yes :tangle no
  def match(partial_data, prior_data):
      if is_variable(partial_data):
          <<match_variable_case>>
      elif is_symbol(partial_data):
          if partial_data == prior_data:
              <<match_symbol_equal_case>>
          else:
              <<match_symbol_not_equal_case>>
      else:
          assert(isinstance(partial_data, list))
          if isinstance(prior_data, list) and len(prior_data) >= len(partial_data):
              #combine the results of matching the individual elements
              <<match_list_case>>
          else:
              <<match_list_no_match>>
#+end_src

The interesting case is when the partial_data is a list.  Let's use the data in example_data_many_matches to guide development of the algorithm.  For now we'll assume matching of the elements occurs via iteration over the list from left to right.  It's not clear why it would be beneficial to do so otherwise, but we'll tag this [DD] (design decision) and can revisit things aren't working.  

Matching elements is kind of tricky because variables in the partial_data can match multiple elements of the corresponding prior_data so we need to keep track of the current position for both lists as we iterate through the partial_data.  

#+name: match_list_case_deprecated
#+begin_src python :noweb yes 
  matches = []
  prior_data_position = 0
  for partial_data_position, partial_data_element in enumerate(partial_data):
      #if there's no more prior data and there is still non-variable partial_data then this is
      #not a match so break
      if prior_data_position >= len(prior_data) and not (is_variable(partial_data_element) and
                                                         partial_data_position == len(partial_data) - 1):
          matches.append(None)
          break
      if is_variable(partial_data_element):
          #this case will need to increment prior_data_position appropriately
          #possibly several steps ahead of partial_data_position
          <<match_list_case_variable_case>>
      else:
          matches.append(match(partial_data_element, prior_data[prior_data_position]))
          prior_data_position += 1
  #if there is still data in prior_data then it's not a match
  
  if prior_data_position < len(prior_data):
      return None
  <<match_list_case_combine_matches>>
  
#+end_src


#+name: count_non_variables
#+begin_src python :noweb yes

#+end_src

At this point it's probably a good idea to look at the different possibilites for the return type of matching so we can fill in the missing pieces.
******** single binding no score
There are a couple ways to handle returning a single match.  For now if there is more than one choice we'll select one at random.

We were last looking at the list case of match, i.e.  matching elements of the partial_data to  elements of the  prior_data.  Here is an example of when the current/first element of the partial_data is a variable.
#+name: variable_case_matching_list_example
#+begin_src python :noweb yes
  partial_data = ['v0', 0, 'v1']
  prior_data = [1,0,1,0,1,0,0,1]
  #bindings for v0
  possible_binding0 = {'v0': [1], 'v1': [1,0,1,0,0,1]}
  possible_binding1 = {'v0': [1,0,1], 'v1': [0,1,0,0,1]}
  possible_binding2 = {'v0': [1,0,1,0,1], 'v1': [0,1]}
  possible_binding3 = {'v0': [1,0,1,0,1,0], 'v1':[1]}
#+end_src
The pattern seems to be v0 can match to any prefix of the prior_data ([1], [1,0,1], [1,0,1,0,1], [1,0,1,0,1,0]) that ends before the next character in the partial_data after the variable (0).

In the case where the current variable element is also the last element then it should be bound to the remaining prior_data.

#+name: match_list_case_variable_case
#+begin_src python :noweb yes 
  if partial_data_position == len(partial_data) - 1:
      matches.append({partial_data_element : prior_data[prior_data_position:]})
      #increment this for the check whether there is more prior_data left even though all
      #partial_data has been iterated through
      prior_data_position = len(prior_data)
  else:
      next_element = partial_data[partial_data_position+1]
      try:
          <<match_list_case_variable_case_has_next_element>>
      except ValueError:
          return None
  
#+end_src

In the case where there is a next element after the current variable element then we can match the variable to any prefix in the prior_data where the element after the last element of the prefix matches the next element in the partial_data, just like in variable_case_matching_list_example for v0.  There is an additional constraint that the remaining prior data after the match must be of greater size than the remaining partial_data and we can ensure this holds by only looking for matches in prior_data up until the position for which the remaining prior_data is the same size as the remaining partial_data.

In the example variable_case_matching_list_example if element is 'v0' then next_element is 0 so we want to find all the positions in prior_data ([1,0,1,0,1,0,0,1]) of 0s e.g. 1, 3, 4, 5, such that there are enough remaining elements in prior_data to match the remaining partial_data. The amount of remaining partial_data is 1 (just the 'v1' element).  If we only look at positions in the prior_data before prior_data[-1:] then any match will have at least 1 element left in prior_data to match the remaining element in the partial_data.

#+name: match_list_case_variable_case_has_next_element_deprecated
#+begin_src python :noweb yes 
  #iterate through the prefixes of prior_data and capture the possible results of matching the variable to it
  variable = partial_data_element
  #remember the possible variable bindings along with the corresponding prior_data_position
  possible_variable_bindings = []
  amount_remaining_needed = len(partial_data[partial_data_position+2:])
  #positions in the prior_data where the prior_data element matches the partial_data element that
  #comes after the variable that is being bound, look at all the positions in prior_data after the current and up to before the tail of prior where the tail is as long as the remaining partial_data
  matching_positions = [i for i in range(prior_data_position+1,len(prior_data[prior_data_position+1:-amount_remaining_needed])+1)
                        if prior_data[i] == next_element]
  
  if len(matching_positions) > 0:
      chosen_match_position = random.choice(matching_positions)
      #bind the prefix of prior_data that ends just before chosen_match_position
      variable_binding = {variable: prior_data[prior_data_position: chosen_match_position]}
      #skip ahead prior_data_position to the point of the match since that will be the next element in the iteration over partial_data
      prior_data_position = chosen_match_position
  
      matches.append(variable_binding)
  else:
      matches.append(None)
#+end_src

We'll need to include the random library when tangling, but what is the best way to do this?
#+name: libs
#+begin_src python :noweb yes :tangle yes
import random
#+end_src

The final part of the match case for lists is combining the matches for the elements in the list.  This means making sure none of the matches were invalid (i.e. had value None) and if everything is ok then merging all the bindings into a single dictionary to be returned.
#+name: match_list_case_combine_matches
#+begin_src python :noweb yes
  if None in matches:
      return None
  else:
      #TODO handle case of same variable appearing in multiple places in the list
      all_bindings = {}
      for bindings in matches:
          all_bindings = dict(all_bindings.items() + bindings.items())
      return all_bindings
#+end_src

The non-list cases for match are fairly straight-forward.
#+name: match_variable_case
#+begin_src python :noweb yes
  return {partial_data: prior_data}
#+end_src


#+name: match_symbol_equal_case
#+begin_src python :noweb yes
  return {}
#+end_src

#+name: match_symbol_not_equal_case
#+begin_src python :noweb yes
  return None
#+end_src

#+name: match_list_no_match
#+begin_src python :noweb yes
  return None 
#+end_src

We need to define a few helper functions in order to run match.  For the type check of whether something is a variable we'll assume for now that variables come in the form 'v' combined with a number e.g. 'v0', 'v142', 'v99'
#+name: is_variable_deprecated
#+begin_src python :noweb yes :tangle no
  def is_variable(thing):
      return isinstance(thing, str) and thing[0] == 'v' \
        and thing[1:].isdigit()
    
#+end_src


#+name: is_variable_test
#+begin_src python :results output :noweb yes
  <<is_variable>>
  
  print is_variable('v0')
  print is_variable('0')
  print is_variable('v302')
  
#+end_src

#+RESULTS: is_variable_test
: True
: False
: True

For symbols we'll assume a symbol is anything not a list.
#+name: is_symbol
#+begin_src python :noweb yes :tangle yes
  def is_symbol(thing):
      return not isinstance(thing, list)
#+end_src
******** multiple bindings no score
******** single binding with score
******** multiple bindings with score

  def match(partial_data, prior_data):
      
  Furthmore we can define types of lists as lists with variables in the top level, lists with variables in a deeper level, and lists without variables.  Now if we pair these possibilities 



TODO look at the case where pattern1 has been added to memory.
******** Returning a set of possible bindings
******* Testing
Now that we have an implementation let's try it on some of the examples from earlier starting with the binary data at the beginning of the document.  Recall we'll be trying to apply match to the partial data of an inference problem and the prior data we've stored in memory.  An example of data in memory could be the bit patterns in pattern0

#+begin_src python :noweb yes :results output 
  <<binary_data>>
  for pattern in pattern0:
      print pattern
#+end_src

#+RESULTS:
: [[1], 0]
: [[1, 1], 0]
: [[1, 1, 1, 1, 1], 0]
: [[1, 1], 0]

An example of prior data from an inference problem could be 
#+begin_src python :noweb yes :results output
  <<inference_problem>>
  <<binary_data_classification_problems>>
  print classify_problem0.partial_data
#+end_src

#+RESULTS:
: [[1, 1, 1, 1], 'v0']

Applying match to a case where it should match and shouldn't match would like the following

#+begin_src python :noweb yes :tangle test_match.py
  from abstraction import *
  from utility import *
  import pdb
  
  test1 =  match([[1, 1, 1, 1], 'v0'], [[1, 1, 1, 1], 0])
  print test1
  assert(test1 == {'v0': [0]})
  test2_0 = match([1, 1, 1, 1], [1, 1, 1, 1, 1])
  print test2_0
  assert(test2_0 is None)
  test2 = match([[1, 1, 1, 1], 'v0'], [[1, 1, 1, 1, 1], 0])
  print test2
  assert(test2 is None)
#+end_src

#+RESULTS
: {'v0': [0]}
: None

We'll also want to look at the cases where matching can have several possibilities such as in variable_case_matching_list_example.

#+begin_src python :noweb yes :tangle test_match.py
  for i in range(10):
      test3 = match(['v0', 0, 'v1'], [1,0,1,0,1,0,0,1])
      print test3
      assert(test3 in [{'v0': [1], 'v1': [1, 0, 1, 0, 0, 1]},
                       {'v0': [1, 0, 1], 'v1': [1, 0, 0, 1]},
                       {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]},
                       {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}])
#+end_src

#+RESULTS
: {'v0': [1, 0, 1], 'v1': [1, 0, 0, 1]}
: {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]}
: {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]}
: {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]}
: {'v0': [1, 0, 1], 'v1': [1, 0, 0, 1]}
: {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}
: {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}
: {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]}
: {'v0': [1], 'v1': [1, 0, 1, 0, 0, 1]}
: {'v0': [1, 0, 1], 'v1': [1, 0, 0, 1]}

Let's take these solutions and substitute them back into the partial data and verify they match the prior data.

#+begin_src python :noweb yes
for i in range(10):
    binding = match(['v0', 0, 'v1'], [1,0,1,0,1,0,0,1])
    print substitute(['v0', 0, 'v1'], binding)
#+end_src

#+RESULTS
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]
: [1, 0, 1, 0, 1, 0, 0, 1]

Now that we have one version of the match function (return single binding, no score), let's move on to complete the loop and return to other versions when the need arises.
****** Problem-solving loop
The original first/naive approach for solving inference problems was to iterate through the prior data and try to find any matches with the partial data.  We'll now formalize this process.  Let's define a class to organize the problem solving functions and data structures.  We'll call it an agent.  It has a memory and a function for taking in inference problems and returning a solution.

#+name: agent
#+begin_src python :noweb yes :tangle no
  class Agent:
      def __init__(self):
          self.memory = Memory()
  
      def solve(self, inference_problem):
          <<agent_solve>>
#+end_src

In order to solve an inference problem, we'll first extract the partial_data then match it against everything in memory.  To keep things simple we'll return a random binding [DD] or None if no solution was found.

#+name: agent_solve_deprecated
#+begin_src python :noweb yes
  solutions = [match(inference_problem.partial_data, prior_data)
               for prior_data in self.memory.memory]
  
  solutions = [solution for solution in solutions if solution != None]
  if solutions == []:
      return None
  else:
      return random.choice(solutions)
#+end_src

We can test the function on the original inference problem examples and binary data.

#+begin_src python :noweb yes :tangle no
    <<binary_data>>
    <<binary_data_classification_problems>>
    <<binary_data_completion_problems>>
  
    agent = Agent()
    [agent.memory.add(data) for data in pattern0+pattern1]
    print 'In memory %s' % agent.memory.memory
  
    problems = [classify_problem0, classify_problem1, complete_problem0, complete_problem1]
    for problem in problems:
        print 'problem: %s' % problem.partial_data
        solution = agent.solve(problem)
        print 'solution: %s' % solution
        if solution is not None:
            print 'completion: %s' % substitute(problem.partial_data, solution)
  
#+end_src

#+RESULTS
: In memory [[[1], 0], [[1, 1], 0], [[1, 1, 1, 1, 1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 1, 1, 1, 0, 0, 0, 0], 1]]
: 
: problem: [[1, 1, 1, 1], 'v0']
: solution: None
: problem: [[1, 1, 1, 0, 0, 0], 'v0']
: solution: {'v0': [1]}
: completion: [[1, 1, 1, 0, 0, 0], 1]
: problem: [[1, 1, 1, 'v0'], 1]
: solution: {'v0': [1, 0, 0, 0, 0]}
: completion: [[1, 1, 1, 1, 0, 0, 0, 0], 1]
: problem: [['v0', 1, 'v1'], 0]
: solution: {'v0': [1, 1, 1], 'v1': [1]}
: completion: [[1, 1, 1, 1, 1], 0]

***** Reflections
We won't go into depth testing this first approach on a wide variety of data since the main point was to work out some of the details for the setup of solving inference problems.  The major limitation of the algorithm was the inability to generalize.  An example was the problem [[1, 1, 1, 1], 'v0'] which intuitively has a solution of {'v0', [1]}.  One possible solution is to use abstraction, which we'll explore in the rest of the document. [add explanation on how abstraction can help]
*** Second Approximation
Our next attempt to solve inference problems will utilize the power of abstraction.  [give some intuitive examples of where abstraction is useful e.g. hierachical planning, etc].  The plan is

1) come up with a few additional inference problems staying with the same two patterns used in binary_data_classfiication_problems and binary_data_completion_problems; these will be used as references while developing the details of the algorithm
2) formally define a process of abstraction based on the lambda abstraction in lambda calculus
3) work out details of how abstraction happens when new data is added to memory as well as how data is stored
4) work out details for matching partial data to possibily abstract data in the memory
5) work out details of how matching is used in the larger problem solving routine

   
**** More Inference Problems
The previous set of problems had one example problem that could not be solved by matching directly to prior data.  We'll give a few more examples here to both guide development and test the abstraction approach.

#+name: additional_binary_classification_problems
#+begin_src python :noweb yes
  classify_problem2 = InferenceProblem(['x0'], [[1,1,1,1,1,1,1,1,1,1,1,1,1], 'x0'])
  classify_problem3 = InferenceProblem(['x0'], [[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0], 'x0'])
  
#+end_src

#+name: additional_binary_completion_problems
#+begin_src python :noweb yes
  completion_problem2 = InferenceProblem(['v0'], [[1,'v0'], 1])
#+end_src

We'll also add in more prior_data to make the patterns more salient.
#+name: additional_prior_data
#+begin_src python :noweb yes
  more_pattern0 = [[[2,2], 0], [[3,3,3,3,3], 0], [[5,5,5,5,5,5,5], 0]]
  questionable_pattern0 = [[[1,0,1,0,1,0,1,0], 0], [[1,2,3,1,2,3,1,2,3,1,2,3], 0]]
  
  more_pattern1 = [[[5,5,5,6,6,6],1], [[3,3,2,2],1]]
#+end_src

And pose problems that test this different type of abstraction.
#+name: surface_level_abstraction_problems
#+begin_src python :noweb yes
  classify_problem4 = InferenceProblem(['v0'], [[8,8,8,8,8], 'x0'])
  classify_problem5 = InferenceProblem(['v0'], [[2,2,2,2,2,2,1,1,1,1,1,1], 'x0'])
  
  classify_problem6 = InferenceProblem(['v0'], [[4,4,5,5,4,4,5,5,4,4,5,5], 'x0'])
  
  completion_problem4 = InferenceProblem(['v0', 'v1'], [[5, 'x0', 6, 'x1'], 1])
  completion_problem5 = InferenceProblem(['v0'], [['x0',9,9,9], 0])
  
  completion_problem6 = InferenceProblem(['v0'], [[1,4,1,1,4,1,1,4,1,'x0',1,4,1], 'x0'])
  completion_problem6 = InferenceProblem(['v0'], [[1,4,1,1,4,1,1,4,1,'x0'], 'x0'])
#+end_src

Another type of pattern we should look at is when the data consists of parts that each have their own structure that may vary, but the relationship between these parts is what matters.  Intuitively digit recognition has is this type of problem where recognizing the character such as 5 is recognizing a flat top part connected to a vertical bar and loop part in a particular way.

From the first patterns one interpretation of pattern1 could be a combination of pattern0 with two different parameters.

#+name: configuration_of_parts_data
#+begin_src python :noweb yes
  #this pattern is 3 patterns that come one after another
  #the first pattern is a constant series
  #the second is an alternating pattern
  #the third is a series of
  pattern2 = [[[8,8,8,8,8,1,0,1,0,1,0,9,9,9,9,0,0,0,0], 1],
              [[7,7,7,3,6,3,6,3,6,3,6,3,6,3,6,3,6,3,6,4,4,4,4,4,0,0,0,0,0], 1]
              [[0,0,0,0,0,0,0,0,0,2,4,2,4,2,4,5,5,5,1,1,1], 1]]
  
  #subpatterns are out of order
  notpattern2 = [[[1,0,1,0,1,0,8,8,8,8,8,9,9,9,9,0,0,0,0], 0],
                 [[4,4,4,4,4,0,0,0,0,0,7,7,7,3,6,3,6,3,6,3,6,3,6,3,6,3,6,3,6], 0],
                 [[0,0,0,0,0,0,0,0,0,5,5,5,1,1,1, 2,4,2,4,2,4,], 1]]
  
#+end_src


**** The process of abstraction
The notion of abstraction we'll be using is very similar to the concept of lambda abstraction in the lambda calculus.  Creating abstractions from data will be similar to the process of anti-unification.  The major adaptation will be related to the fact that variables can be replaced with multiple elements that get inserted into their place instead of a single element (see the definition of substitution above).  

Let's first define a data structure for an abstraction.  It is basically the same thing as an inference problem: a set a variables and some data (list, possibly nested) that contain those variables.

#+name: abstraction
#+begin_src python :noweb yes
  Abstraction = InferenceProblem
#+end_src

The basic intuition is abstractions can be made from data by saving what is common between the data and replacing what is different with variables.  The abstraction can then be used to help solve inference problems in a similar way to how the complete prior data in our first approach helped.  If the non-variable parts of the inference problem match to the abstraction and the abstraction has non-variable parts that match up to the variable parts of the partial data then we can bind these (just as in the case where we match complete data to partial data).  The difference (and where the flexibility comes in) is that unlike when matching to complete data not every part of the non-variable partial data has to have an exact counterpart in the abstraction since abstractions also have variables.

So for example suppose our inference problem has the following partial data (with variables can be determined by assumption about their form we used earlier)

[[1, 1, 1, 1], 'v0']

and now suppose in our memory we have the following abstraction

[[1, 'v0"'], 0]

We can match the first part of the first elements then bind to v0" the remaining [1,1,1] of the inference problem's first element.  Now since we first element did not have any conflict in matching we can bind the second element of the inference problem v0 to the abstraction's second element 0.  The important thing is more than one inference problem of the same form can be solved with this abstraction e.g. [[1,1], 'v0'], [[1,1,1,1,1,1,1], 'v0'], etc.  unlike in our first approach where a complete data such as [[1,1,1], 0] could only solve one of these inference problems, specifically [[1,1,1], 'v0'].

Another useful thing about having abstractions in the prior data is that one can perform inference within the prior data which can then be used to help inference in the problem data.  A simple example is as follows.  Suppose the inference problem is

[1,0, 'v0']

and the abstraction in our prior data is

['v0"', 0, 'v0"']

Observe we don't have any concrete element that matches the variable in the partial data in the abstraction, but we can "infer" the first v0" is 1 based on data in the inference problem, then we can use this to say the second v0" is 1 which we can then use create a binding for v0 to 1 and give use a possible solution to the inference problem.

The main things we'll need to work out the details for are how do useful abstractions get created from data and what is the matching process like between an abstraction and inference problem.

***** Creating abstractions
In this section we explore the process of creating abstractions from data.  There are three cases we will look at
1) abstracting from concrete data and concrete data
2) abstracting from concrete data and abstraction
3) abstracting from abstraction and abstraction

The basic idea is to do something like anti-unification where we try to create new data similar to the original data, but with the differences replaced by variables.
****** Abstracting from concrete data and concrete data
Suppose we have the data from pattern0
#+begin_src python :noweb yes
  pattern0 = [[[1], 0], [[1,1], 0], [[1,1,1,1,1], 0], [[1,1], 0]]
#+end_src

If we start with the first piece of data, [[1], 0], in our memory and add the second, [[1,1], 0], what kind of abstractions are reasonable?

One possibility is to note the first element is different and get ['v0', 0]. Another possibility is to compare [1] and [1,1] and create the abstraction [1, 'v0'] and eventually get [[1, 'v0'], 0].  

==Notes on allowing the empty list to be bound to a variable
In the first approximation, the match function assumed variables could not be bound to empty lists, while the second abstraction [[1,'v0'], 0] does assume this.  This will probably make matching more difficult since it increases the number of possible matches.

If a variable can be bound to the empty list then a variable can be placed pretty much anywhere in the data and create a valid abstraction e.g. [['v0', '1, 'v1'], 'v2', 0, 'v3'] can have the binding {'v0': [], 'v1': [1], 'v2': [], 'v3': []} and the substitution would be the second example of pattern0 data [[1,1], 0].
==

Comparing other examples of the pattern0 data would yield similar results e.g. anything compared with [[1], 0] would get the same abstractions and comparing [[1,1], 0] with [[1,1,1,1,1], 0] gives us ['v0', 0] and [[1,1,'v1'], 0] as intuitive abstractions.

Let's do this excercise with pattern1
#+begin_src python :noweb yes
 pattern1 = [[[1,1,0,0],1], [[1,1,1,0,0,0],1], [[1,1,1,1,0,0,0,0],1]]
#+end_src

Comparing [[1,1,0,0],1] with [[1,1,1,0,0,0],1], some reasonable abstractions include
['v0', 1]

[[1,1,'v0'],1]

[[1,1,'v0',0,0,'v1'], 1]

The main difficulty of creating abstractions will be determining the appropriate places to insert variables due to the large number of possibilities.

******* Algorithm Description
The examples from above combined with previous implementations of anti-unification suggest a recursive algorithm for creating abstractions.  If the data are not lists, return the data if they are equal otherwise a variable.  If the data are lists then iterate through the elements and try to create abstractions between them.  This is the tricky part since we need to figure out how to insert variables appropriately into the resulting list.

==Notes on return type
It might make sense to simplify things by not using the class definition for inference problem and abstraction.  If we assume variables have a certain form (which we have been doing already) then inference problems and abstractions are just lists with these special symbols.

For now we'll keep the classes, but we'll have abstract just return the partial_data part (since it's easier to do that with a recursive implementation) then we'll have a function that wraps the partial data in an Abstraction class.

Eventually we will want named abstractions and be able to have lists where the elements are the names of other abstractions along with parameters for those abstractions so having the classes then will be useful, but we can wait for the third approximation or beyond.
==

#+name: abstract
#+begin_src python :noweb yes :tangle yes
  def abstract(data1, data2):
      if not isinstance(data1, list) or not isinstance(data2, list):
          if data1 == data2:
              return data1
          else:
              return variable_generator.generate()
      else:
          #both data are lists so iterate through abstracting the elements together
          <<abstract_list_case>>
#+end_src

For abstracting two lists we need to know when to insert variables and how much of each list the variable should match up to.  In the case of pattern1 abstracting from [[1,1,0,0],1] and [[1,1,1,0,0,0],1] we first check if both lists are the same length, if they are we can pair the elements and abstract them e.g. [1,1,0,0] with [1,1,1,0,0,0] and 1 with 1.

In the case of [1,1,0,0] with [1,1,1,0,0,0] they are not the same length so we have some flexibility as to where variables can go.  The goal is to come up with an abstraction such that both lists are instances of the abstraction.  One possibility is to take the smaller list and try to insert variables such that there is a binding that makes it equal to the larger list.  An element in the smaller list that isn't in the larger can be replaced with a variable.  In general we want to find as many elements that match in the right order then try to insert variables into the resulting list such that there are substitions that create the original lists.  For now we'll develop something ad-hoc and come back to this with a more optimal soluction if it is important [ALGORITHMIC PROBLEM].  One reason it might not be that important to find the optimal abstraction for any pair of data is that good abstractions should be promoted in the outer loop so given many pairs of data the best ones will be found eventually.  It might be worth creating random abstractions to reduce any bias... what are good algorithms for picking out random abstractions?

One algorithm is to take the smaller list and for each element find the first element in the larger list that matches.  Replace any gap with a variable or if there is no match for the current element then replace it with a variable.

For example with SL := [1,1,0,0] and BL := [1,1,1,0,0,0] we iterate through the elements of SL starting with SL[0] that matches to BL[0] then SL[1] matches to BL[1], SL[2] does not match BL[2] so we start binding values in BL to a new variable SL[2] matches to BL[3], SL[3] matches to BL[4] and then SL is out of elements so we create a variable that will bind to the remaining elements of BL (BL[5]).

Let's look at another case where there are non-matching elements in the smaller list e.g. SL=[1,2,2,1] and BL=[1,1,1,1,1] we start by matching SL[0] with BL[0] then find SL[1] does not match BL[1] so we start binding elements of BL to a new variable v0 looking for an element to match to SL[1], but we do not so instead of having a variable between SL[0] and SL[1] we guess SL[1] should be a variable and we'll bind it to BL[1] as kind of an arbitrary design decision [DD].  A similar thing happens with SL[2] which is turned into a variable that binds to BL[2].  SL[3] binds to BL[3] and a variable is created to bind to the remaining BL[4] resulting in [1,v0, v1, 1,v2], but we know we can remove any variables that are next to each other so we get [1,v0,1,v2].  It seems better to we had been able to reach the abstraction [1,v0,1] since there are less variables.


#+name: abstract_list_case
#+begin_src python :noweb yes 
  def fast_forward(position_in_big, big_list, element_in_small):
        for new_position, element_in_big in enumerate(big_list[position_in_big:], start=position_in_big):
            if element_in_big == element_in_small:
                return new_position+1
        return None
  
  element_abstractions = []
  small_list,big_list = sorted([data1,data2], cmp=lambda x,y: len(x)-len(y))
  position_in_big = 0
  for position_in_small, element_in_small in enumerate(small_list):
      element_abstraction = abstract(element_in_small, big_list[position_in_big])
      element_abstractions.append(element_abstraction)
      if is_variable(element_abstraction):
          #fast forward position_in_big until the next element will match element_in_small
          #i.e. try and squeeze non-matching segments in the big list in between matching
          #parts of the small list
          new_position_in_big = fast_forward(position_in_big, big_list, element_in_small)
          if new_position_in_big is None:
              position_in_big += 1
          else:
              position_in_big = new_position_in_big
              element_abstractions.append(element_in_small)
      else:
          position_in_big += 1
  #if there are more elements left in the big list bind them to a variable, observe
  #position_in_big was incremented one last time so we check it against len(big_list) instead
  #of len(big_list)-1 the position of the last element in big_list
  if position_in_big < len(big_list):
      element_abstractions.append(variable_generator.generate())
  return element_abstractions
  
#+end_src

#+name: generate_variable 
#+begin_src python :noweb yes :tangle yes
  class VariableGenerator:
      def __init__(self):
         self._counter = 0
  
      def generate(self):
        new_variable = 'v'+str(self._counter)
        self._counter += 1
        return new_variable
  
  variable_generator = VariableGenerator()
#+end_src

It's worth noting this algorithm has a certain amount of "bias." An example of when this algorithm performs poorly is [1,0,0,0,0] and [0,0,0,0,0,0,0,1], which would result in ['v0', 1, 'v1'] where v0 binds to [] and [0,0,0,0,0,0,0] (in the first and second lists respectively) and v1 binds to [0,0,0,0] and [].  One possible "fix" is to run the above algorithm repeatedly, but each time starting at the next element over e.g. first starting at list[0] then list[1] then list[2] etc using a variable to in front of the starting point to capture and non-matches.  Then take the best match based on the repeated runs.  Are there still poor performing cases (?) Probably... better to let the data bias the abstractions than the algorithms (add more randomness to list abstraction case, let good abstractions become prominent in the outer loop which matches prior data to indference problem partial data).

What are we trying to optimize?  The above examples seem to indicate we'd like to maximize the number of non-variables while minimizing the number of variables in the resulting abstraction (is this true?  in the case of [1,1,0,0] and [1,1,1,0,0,0] we could get fewer variables with [1,1,v0,0,0] but intuitively having two distinct variables with different "types" seems to fit better)


We can test the algorithm on our worked through example to make sure it's what we expect.

#+begin_src python :noweb yes :tangle test_abstract.py
  from abstraction import *
  <<binary_data>>
  abstraction = abstract(pattern1[0], pattern1[1])
  print abstraction
  assert(abstraction == [[1, 1, 'v0', 0, 0, 'v1'], 1])
  abstraction2 = abstract(pattern0[0], pattern0[1])
  print abstraction2
  assert(abstraction2 == [[1, 'v2'], 0])
#+end_src

#+begin_src python :noweb yes :tangle test_match.py
  <<binary_data>>
  abstraction = [[1, 1, 'v0', 0, 0, 'v1'], 1]
  print match(abstraction, pattern1[0])
  print match(abstraction, pattern1[1])
#+end_src

******** Changing match
The reason match fails to come up with an appropriate binding is we assumed earlier that the binding for a variable could not be an empty list.  Now that we have formalized the process of abstraction more we find being able to bind a variable to an empty list is useful, so let's adjust the necessary parts of the match function.

It will help to look at an example of how match would work when a variable can be bound to the empty list.  We'll use 

#+name: 
#+begin_src python :noweb yes
  partial_data = [[1, 1, 'v0', 0, 0, 'v1'], 1]
  prior_data = [[1,1,0,0],1]
#+end_src

Stepping through the code the first thing we realize is the constraint that the length of the prior_data needs to be greater than the length of the partial_data is no longer true.  Let's remove that condition

#+name: structure_for_match_deprecated
#+begin_src python :noweb yes :tangle no
  def match(partial_data, prior_data):
      if is_variable(partial_data):
          <<match_variable_case>>
      elif is_symbol(partial_data):
          if partial_data == prior_data:
              <<match_symbol_equal_case>>
          else:
              <<match_symbol_not_equal_case>>
      else:
          assert(isinstance(partial_data, list))
          if isinstance(prior_data, list):
              #combine the results of matching the individual elements
              <<match_list_case>>
          else:
              <<match_list_no_match>>
#+end_src



The next place that changes is match_list_case_variable_case_has_next_element.  Previously we used a constraint when matching the element after the variable in the partial_data that the amount of prior_data after the match had to be greater than the amount of partial_data after the match, but this is no longer the case since the prior_data does not have to be longer than the partiaL_data.  

A condition that does need to hold is for the number of non-variables after the chosen match in the prior_data should be at least as many as the non-variables in the partial_data after the variable. One thing to consider thougth is whether checking for this condition guarantees if a binding exists then the match will find it.  If it doesn't then it might be worth keeping the code simpler for now and optimizing (for correctness in this case) after the whole system is built since not having a correct answer each time might not be that important (e.g. there may be handling of fault tolerance elsewhere).

Let's try to find a case where match could fail to produce a valid binding.  The main point where failure could occur is  probably when the variable match is chosen at random.  We need to find a case where one of the possible matches makes it so there is no match as we continue on.  We'll start by trying to use the simplest data possible to try and create this case.  Let the partial_data ['v0', 1, 0] and prior_data be [1,1,1,0].  'v0' can bind to the first two 1s.    What about ['v0', 1, 'v1', 0] then we can get {v0: [1,1]} {v1:[]}.  At this point perhaps if there is a valid binding then it is possible to choose randomly (as long as you leave enough non-variables) and always get a valid binding.  The reason roughly is if there is a valid binding then any remaining non-variables in the partial_data will have a match in the remaining prior_data (why?).

#+name: match_list_case_variable_case_has_next_element
#+begin_src python :noweb yes 
  #iterate through the prefixes of prior_data and capture the possible results of matching the variable to it
  variable = partial_data_element
  #remember the possible variable bindings along with the corresponding prior_data_position
  possible_variable_bindings = []
  
  #positions in the prior_data where the prior_data element matches the partial_data element that
  #comes after the variable that is being bound, look at all the positions in prior_data after the current and up to before the tail of prior where the tail is as long as the remaining partial_data
  matching_positions = find_matching_positions(next_element, prior_data, partial_data,
                                               prior_data_position, partial_data_position)
  
  if len(matching_positions) > 0:
      chosen_match_position = random.choice(matching_positions)
      #bind the prefix of prior_data that ends just before chosen_match_position
      variable_binding = {variable: prior_data[prior_data_position: chosen_match_position]}
      #skip ahead prior_data_position to the point of the match since that will be the next element in the iteration over partial_data
      prior_data_position = chosen_match_position
  
      matches.append(variable_binding)
  else:
      matches.append(None)
#+end_src
In order to find the possible matching positions we need to check two things as we iterate through the prior_data.

1) The next_element in the partial_data matches the current prior_data element.
2) There are at least as many non-variables after the current prior_data element as there are non-variables in the remaining partial_data.

Just like the last version of match we can determine how much of prior_data we need to iterate through to guarantee 2) without having to check for each prior_data element.  We need only determine the number of non-variables in the remaining partial_data then find the tail of the prior_data that contains that number of non-variables and only iterate until the beginning of the tail.
#+name: find_matching_positions_deprecated
#+begin_src python :noweb yes :tangle no
  def find_matching_positions(next_element, prior_data, partial_data,
                              prior_data_position, partial_data_position):
      def find_last_prior_position(number_non_variables):
          count_down_position = len(prior_data)-1
          while number_non_variables > 0 and count_down_position > prior_data_position:
              if not is_variable(prior_data[count_down_position]):
                  number_non_variables -= 1
              count_down_position -= 1
          return count_down_position
  
      non_variables_in_partial = [element
                                  for element in partial_data[partial_data_position+1:]
                                  if not is_variable(element)]
      #find last position in prior_data where remaining elements have same number of
      #non-variables as non_variables_in_partial-1, the -1 is because we don't count
      #the first non_variable which is the next_element
      assert(non_variables_in_partial[0] == next_element)
      last_prior_position = find_last_prior_position(len(non_variables_in_partial)-1)
  
      matching_positions = [i for i,element
                            in enumerate(prior_data[prior_data_position:last_prior_position+1],
                                         start=prior_data_position)
                            if match(next_element, element) is not None]
  
      return matching_positions
  
#+end_src

Let's test the update match on a case where the algorithm can fail.
#+name: match_fail
#+begin_src python :noweb yes :tangle test_match.py
  for i in range(10):
      #a case where this algorithm can fail to find the existing valid binding
      test4 = match(['v0', 0], [1,0,1,0,1,0,0])
      print test4
      assert(test4 in [None, {'v0': [1, 0, 1, 0, 1, 0]}])
#+end_src

#+RESULTS
: {'v0': [0]}
: None
: {'v0': [1, 0, 1], 'v1': [1, 0, 0, 1]}
: {'v0': [1], 'v1': [1, 0, 1, 0, 0, 1]}
: {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]}
: {'v0': [1], 'v1': [1, 0, 1, 0, 0, 1]}
: {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}
: {'v0': [1, 0, 1, 0, 1], 'v1': [0, 1]}
: {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}
: {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}
: {'v0': [1, 0, 1], 'v1': [1, 0, 0, 1]}
: {'v0': [1, 0, 1, 0, 1, 0], 'v1': [1]}
: None
: None
: None
: {'v0': [1, 0, 1, 0, 1, 0]}
: None
: None
: None
: None
: None
: None

For the most part the fixed match meets our expectation, but we also found a case where this algorithm can fail to find a valid binding if it exists.  Rather than special case a fix, we'll wait to resolve this issue when we begin to look at match between abstractions.

****** [SKIP] Abstracting from concrete data and abstract data
****** [SKIP] Abstraction from abstraction and abstraction
***** Using abstractions
We have a way of creating abstractions from data, but how is this used in the process of solving inference problems?  We'll extend how data is added to memory as well as how inference problems are solved by an agent by incorporating abstraction.


****** [SKIP] Storing abstractions
Now that abstractions can be part of the prior data when we add something new to memory should we try and write it in terms of existing abstractions?  How might storing data in terms of existing abstractions help to solve inference problems?  Perhaps efficiency?  Storage space.

Let's walk through what the storage process might look like for the data we've been using in our examples

******* Adding data from pattern1 and pattern2
******** Pattern0[0] and Pattern1[0] 
#+begin_src python :noweb yes :results output
<<binary_data>>
print 'pattern0:%s' % pattern0
print 'pattern1:%s' % pattern1
#+end_src

#+RESULTS:
: pattern0:[[[1], 0], [[1, 1], 0], [[1, 1, 1, 1, 1], 0], [[1, 1], 0]]
: pattern1:[[[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 1, 1, 1, 0, 0, 0, 0], 1]]


Suppose we added data alternating between pattern0 and pattern1 and when we add data we try to generate abstractions.

The first abstraction we'd get is 
#+begin_src python :noweb yes :results output
  from abstraction import abstract
  <<binary_data>>
  
  print abstract(pattern0[0], pattern1[0])
#+end_src

#+RESULTS:
: [[1, 'v0'], 'v1']

If we name the abstraction e.g. f, we could use it to compress the data we've already seen getting

['f' [] [0]]
['f' [1, 0, 0] [1]]

instead of 

[[1], 0]
[[1, 1, 0, 0], 1]

******** Pattern0[1]
Now suppose the next data we get is [[1, 1], 0].  What are possible things we could do when adding this to memory.  We could first see if it's an instantiation of an abstraction we already know.  This can be done by applying match with all the known abstractions.

#+begin_src python :noweb yes :results output :tangle test_match.py
  #the abstraction f
  f = [[1, 'v0'], 'v1']
  test5 = match(f, [[1,1],0])
  print test5
  assert(test5 == {'v0': [1], 'v1': [0]})
#+end_src

#+RESULTS:
: {'v0': [1], 'v1': [0]}

which means we could rewrite the new data as
[f [1] [0]]

Now we can try and create new abstractions
#+begin_src python :noweb yes :results output
  from abstraction import *
  
  print abstract(['f', [1], [0]], ['f', [], [0]])
  print abstract(['f', [1], [0]], ['f', [1, 0, 0], [1]])
#+end_src

#+RESULTS:
: ['f', ['v0'], [0]]
: ['f', [1, 'v1'], ['v2']]

Intuitively the first abstraction ['f', ['v0'], [0]] seems like it captures some of the structure of pattern0.  One interpretation is it is a list of two things and the first element is of the first is 1 and the second element is 0.  The second says all the data consists of two elements where the first element starts with two ones.

Perhaps it would also make sense to try and combine the two abstractions into a single abstraction e.g. ['f', ['v0'], [0]] and [[1, 'v0'], 'v1'] could become [[1, 'v0'], [0]].  We'll hold off on exploring the tradeoffs of doing such a thing until later though [DD].

******** Pattern1[1]
Now let's add [[1, 1, 1, 0, 0, 0], 1] to memory.

We'll try to first rewrite it in terms of any known abstractions.  
********* Rewriting in terms of existing abstractions
#+begin_src python :noweb yes :results output :tangle test_match.py
  f = [[1, 'v0'], 'v1']
  g = ['f', ['v0'], [0]]
  h = ['f', [1, 'v1'], ['v2']]
  
  test6 = match(f, pattern1[1])
  print test6
  assert(test6 == {'v0': [1, 1, 0, 0, 0], 'v1': [1]})
  test7 = match(g, pattern1[1])
  print test7
  assert(test7 is None)
  test8 = match(h, pattern1[1])
  print test8
  assert(test8 is None)
#+end_src

#+RESULTS:
: {'v0': [1, 1, 0, 0, 0], 'v1': [1]}
: None
: None

One interesting thing to note is in the current system we can only ever match new data to abstractions that have been created directly from the non-partial data.  So e.g. g and h don't match because the 'f' symbol only occurs internally and would never appear in external data.  One question is whether it would be useful to try and match higher level abstractions to low-level data.  Intuitively this seems like trying to find parts of an object first then recognize a whole as opposed to trying to find the unchanging structure of the whole first then determining the possibly changing parts.

What we mean by trying to find the parts of the whole is kind of vague, but one can get a sense by looking at match(['f', ['v0'], [0]], [[1, 1, 1, 0, 0, 0], 1]).  If one were to redefine match to special case function applications, it seems like you would try to find where the arguments to the application ('v0' and 0 in the example) matched the data and then if matches were found you could try and see if those pieces related to each other in the right way by looking at the body of the function ('f' in this case).  Looking at this example it seems not that useful so we'll ignore it for now, but keep it in mind as a potential place for parrallelization [DD] [P] [E].  ([E] will be used to indicate possible places to improve efficiency)

The past two paragraphs indicate we can structure the abstractions in a way that we don't have to try all of them when checking to see if some data is an instance of it. [E]  

We have one match so we can rewrite the new data as ['f', [1, 1, 0, 0, 0], [1]].  In this form we see we can attempt to further rewrite the expression in terms of some of the higher-order (right term?) abstractions.

#+begin_src python :noweb yes :results output :tangle test_match.py
  from abstraction import *;
  pattern1_1 = ['f', [1, 1, 0, 0, 0], [1]]
  
  g = ['f', ['v0'], [0]]
  h = ['f', [1, 'v1'], ['v2']]
  
  print_and_assert_equal(match(g, pattern1_1), None)
  print_and_assert_equal(match(h, pattern1_1), {'v1': [1, 0, 0, 0], 'v2': [1]})
#+end_src

#+RESULTS:
: None
: {'v1': [1, 0, 0, 0], 'v2': [1]}

This gives us ['h', [1, 0, 0, 0], [1]] so we save an extra character.

Now that we've finished rewriting pattern1[1] in terms of abstractions already in memory, we can try and create new abstractions from it.
********* creating new abstractions 
The current data in memory is the following:

['h', [1, 0, 0, 0], [1]]
['f', [1], [0]]
['f', [], [0]]
['f', [1, 0, 0], [1]]

with abstractions
f = [[1, 'v0'], 'v1']
g = ['f', ['v0'], [0]]
h = ['f', [1, 'v1'], ['v2']]

Let's see what kind of abstractions we get from ['h', [1, 0, 0, 0], [1]].

#+begin_src python :noweb yes :results output

  from abstraction import *
  pattern1_1 = ['h', [1, 0, 0, 0], [1]]
  pattern0_1 = ['f', [1], [0]]
  pattern0_0 = ['f', [], [0]]
  pattern1_0 = ['f', [1, 0, 0], [1]]
  
  print abstract(pattern1_1, pattern0_1)
  print abstract(pattern1_1, pattern0_0)
  print abstract(pattern1_1, pattern1_0)
#+end_src

#+RESULTS:
: ['v0', [1, 'v1'], ['v2']]
: ['v3', ['v4'], ['v5']]
: ['v6', [1, 0, 0, 'v7'], [1]]

********* Rewriting in terms of the new abstractions
If we try to rewrite the data in terms of the new abstractions we get

#+begin_src python :noweb yes :results output :tangle test_match.py
  from abstraction import *
  new_abstractions = [['v0', [1, 'v1'], ['v2']],
                      ['v3', ['v4'], ['v5']],
                      ['v6', [1, 0, 0, 'v7'], [1]]]
  pattern0 = [['f', [], [0]], ['f', [1], [0]]]
  pattern1 = [['f', [1, 0, 0], [1]], ['h', [1, 0, 0, 0], [1]]]
  
  for pattern in pattern0:
      for abstraction in new_abstractions:
          print '%s,%s =>' % (abstraction, pattern)
          print match(abstraction, pattern)
          print '\n'
  
  for pattern in pattern1:
      for abstraction in new_abstractions:
          print '%s,%s =>' % (abstraction, pattern)
          print match(abstraction, pattern)
          print '\n'
#+end_src

#+RESULTS:
: {'v1': [1, 0, 0, 0], 'v2': [1]}
: ['v0', [1, 'v1'], ['v2']],['f', [], [0]] =>
: None
: 
: 
: ['v3', ['v4'], ['v5']],['f', [], [0]] =>
: {'v3': ['f'], 'v4': [], 'v5': [0]}
: 
: 
: ['v6', [1, 0, 0, 'v7'], [1]],['f', [], [0]] =>
: None
: 
: 
: ['v0', [1, 'v1'], ['v2']],['f', [1], [0]] =>
: {'v0': ['f'], 'v1': [], 'v2': [0]}
: 
: 
: ['v3', ['v4'], ['v5']],['f', [1], [0]] =>
: {'v3': ['f'], 'v4': [1], 'v5': [0]}
: 
: 
: ['v6', [1, 0, 0, 'v7'], [1]],['f', [1], [0]] =>
: None
: 
: 
: ['v0', [1, 'v1'], ['v2']],['f', [1, 0, 0], [1]] =>
: {'v0': ['f'], 'v1': [0, 0], 'v2': [1]}
: 
: 
: ['v3', ['v4'], ['v5']],['f', [1, 0, 0], [1]] =>
: {'v3': ['f'], 'v4': [1, 0, 0], 'v5': [1]}
: 
: 
: ['v6', [1, 0, 0, 'v7'], [1]],['f', [1, 0, 0], [1]] =>
: {'v6': ['f'], 'v7': []}
: 
: 
: ['v0', [1, 'v1'], ['v2']],['h', [1, 0, 0, 0], [1]] =>
: {'v0': ['h'], 'v1': [0, 0, 0], 'v2': [1]}
: 
: 
: ['v3', ['v4'], ['v5']],['h', [1, 0, 0, 0], [1]] =>
: {'v3': ['h'], 'v4': [1, 0, 0, 0], 'v5': [1]}
: 
: 
: ['v6', [1, 0, 0, 'v7'], [1]],['h', [1, 0, 0, 0], [1]] =>
: {'v6': ['h'], 'v7': [0]}
: 
One major question this example brings up is how do we know which data should be rewritten in terms of a new abstraction.  Intuitively pattern1 examples should be rewritten in terms of abstractions generated from their data, but how do we recognize this?  One criteria could be acompression.  Probably the main one though is performance on inference, if abstractions are not useful for inference then they should be removed.  

Optimizing for compression may also optimize for inference because in some sense you are making this assumption/hope that new data will have common structure with prior data and that is why you can perform inference.  If you have good compression it means you've extracted a lot of the structure/commonality of the existing data out and because of your assumption this can be used when inferring something about new data.

******* Formalizing the problem/process of storing abstractions
******** Enumeration of abstractions
What are the possible ways to use abstraction as new data comes in?  The items on this list are not mututally exclusive.

1) run abstract with the new data against everything in memory
2) try to rewrite the new data in terms of existing abstractions
3) try to rewrite and replace old data in terms of new abstractions created by the new data
4) try to rewrite but not replace old data in terms of new abstractions created by the new data

The brute force thing to do would be every time a new piece of data comes in run abstract with it against everything in memory (both abstractions and data). Add every new abstraction created to memory.  For each new abstraction 1) treat it as if it were new data entering memory (so start from the beginnning) 2) try to rewrite everything in memory in terms of the abstraction, add any new rewrite to memory and treat it as a new data entering memory

This would give us a huge amount of data and possibly infinite running process (are these assertions true?) [TQ] TQ for theory question, but for inference we would have everything in the prior data that could possibly be useful [TQ].

Maybe we should have individual processes always running for abstracting and rewriting and we could balance these out with processes that are at the same time removing things from memory. Ultimately we need a better sense of how abstractions can be used to solve inference problems to guide the design of what is stored in memory, but we'll first start with a hypothesis on how to store it then let the inference performance determine how to adjust things.
******** Compression as a criteria for selecting abstractions
One heuristic for determing what abstractions to create and use is to try and minimize the size of memory.  A straight-forward algorithm for this would be



******* Recognizing pattern1 in terms of pattern0
Intuitively it seems like we want to be able to recognize parts of new data in terms of previously recognized abstractions.  In this section we look at how the match algorithm might do this and try to better understand why this would be a good thing (if it is).

We'll start by looking at pattern0 and its relation to pattern1 with the idea being that pattern1 is two pattern0s put together with the same length, but different characters.  

******** abstractions for pattern0
Informally pattern0 can be described as any list where each element is the same character.  Examples we've listed so far are

#+begin_src python :noweb yes :results value
  <<binary_data>>
  <<additional_prior_data>>
  
  return pattern0+more_pattern0
#+end_src

#+RESULTS:
| (1)             | 0 |
| (1 1)           | 0 |
| (1 1 1 1 1)     | 0 |
| (1 1)           | 0 |
| (2 2)           | 0 |
| (3 3 3 3 3)     | 0 |
| (5 5 5 5 5 5 5) | 0 | 

What are the abstractions we get from this data?

#+name: 
#+begin_src python :noweb yes :tangle pattern0.py
  from abstraction import *
  
    <<binary_data>>
  <<additional_prior_data>>
  
  abstract(pattern0[0], 
  
#+end_src  





Walk through how matching partial data to a part of prior data would work

Show how this can get better compression than not rewriting pattern1 in terms of pattern0

******* Adding data from pattern2

One question we should address when adding new data to the memory is how one can match prior data to only a part of the new data e.g. writing the new data in terms of multiple abstractions.

The main question is how does this help with inference?  One reason is having more partial data in memory means given an inference problem there is more chance of getting a partial match will let us infer the unknown based on prior data (of course the more partial data there is the more one would have to search to find a match).  Let's add a bit more data and then try to see how the stored structures can be used to solve inference problems.

****** [SKIP] Solving inference problems with abstraction
Earlier we looked at one example of how abstraction can help solve an inference problem (the section following the code block named abstraction).  Let's look at some of the other additional inference problems we listed and see how abstraction might be used to solve them.  

******* The Utility of Abstraction
******** Compression
Abstractions can save space in memory by removing redundancy.
******** Hierarchical Search
How can abstractions in memory help in solving inference problems?  Multiple levels of abstraction can 
******* classify_problem2
******* classify_problem3
******* completion_problem2
****** Solving classify_problem0 with abstraction
******* Manual solution
The complexity of possible approaches being explored in "Storing abstractions" and "Solving inference problems with abstraction" seems like it could be a bit premature and risks creating the wrong generalizations.  This section is a second take to developing the algorithms for using abstraction by focusing on only adding enough complexity to solve the classify_problem0, which we failed to solve with our first approach and eventually examples presented in the section "More Inference Problems".  We start with the function for creating abstractions developed in "Creating abstractions" and ask what is the simplest modification to the previous system that can enable it to solve classify_problem0.

Let's start by incrementally processing data from pattern0 and pattern1 and trying to solve classify_problem0 at each step.

We initialize the agent and try to solve the inference problem with no prior data.

#+begin_src python :noweb yes :results output :tangle second_approximation.py
  from abstraction import *
  from utility import *
  <<binary_data>>
  <<binary_data_classification_problems>>
  
  agent = Agent()
  print 'classify_problem0 %s' % classify_problem0.partial_data
  
  print_and_assert_equal(agent.solve(classify_problem0), None)
#+end_src

#+RESULTS:

=======Literate Programming aside======
One thing we have been doing is to print output as we go along then use this to create test cases, which help verify that any future changes to the code are correct.  One way to streamline this is to have a function that both prints and asserts.  We'll start to build up generic utility functions in a file utility.py

#+name: print_and_assert_equal
#+begin_src python :noweb yes :tangle utility.py
  def print_and_assert(expression_text, operator, expression_output, assert_value):
      print '%s => %s' % (expression_text, expression_output)
      assert(operator(expression_output, assert_value))
  
  def equal(a,b):
      return a == b

  def print_and_assert_equal(function_output, true_value):
      print_and_assert('equality check', equal, function_output, true_value)
#+end_src
=======================================
Next we add pattern0[0] and try to solve the problem again.

#+begin_src python :noweb yes :tangle second_approximation.py
  agent.memory.add(pattern0[0])
  
  <<binary_data_classification_problems>>
  print agent.memory.memory

  print_and_assert_equal(agent.solve(classify_problem0), None)
#+end_src

#+RESULTS:
: [[[1], 0]]
: None

As expected there isn't enough data to solve the problem or generalize.  If we add pattern0[1] things should get interesting.  Let's first add it and attempt to solve.

#+begin_src python :noweb yes :tangle second_approximation.py
  agent.memory.add(pattern0[1])
  print agent.memory.memory
  
  print_and_assert_equal(agent.solve(classify_problem0), None)
#+end_src

#+RESUTS:
: [[[1], 0], [[1, 1], 0]]
: None

Now that we have two pieces of data we can try to generalize.

#+begin_src python :noweb yes :tangle second_approximation.py
  print_and_assert_equal(abstract(agent.memory.memory[0], agent.memory.memory[1]), [[1, 'v0'], 0])
#+end_src

#+RESULTS:
: [[1, 'v0'], 0]

At this point there is a possibility to rewrite the data that was used to create the generalization in terms of the generalization.  For now we'll hold off on doing that since 1) it adds the complexity of naming abstractions and 2) it's not yet clear how that would help inference.

Let's see how we need to modify the existing match function can be used to solve the inference problem in the way it was described at the introduction of "The process of abstraction".

#+begin_src python :noweb yes :tangle second_approximation.py
  print match(classify_problem0.partial_data, [[1, 'v0'], 0])
#+end_src

#+RESULTS:
: None

The reason match currently fails to find a binding is variables in the prior_data argument are treated like constants and so 'v0' in the abstraction fails to match the partial_data.

How should we treat variables in the prior_data argument to match when the prior_data is an abstraction?  Should match try to return an environment that makes the resulting instances valid?  Do we have to worry about variables shared across different abstractions or naming conflicts?

Let's try to re-examine the purpose of match to guide our design.  Originally the goal of match was to find a set of bindings that would make the prior_data an instance of the partial_data.  A natural generalization seems like match should return an environment that instantiates any abstractions to the same value.  Some concrete examples are

#+begin_src python :noweb yes
  #variables do not overlap, a match exists
  print_and_assert_equal(match([[1,1,1,1], 'v0'], [[1, 'v1'], 0]), {'v0': [0], 'v1': [1,1,1]})
  #one abstraction is more general than the other
  print_and_assert_equal(match(['v0'], [[1], 'v1'], {'v0': [[1], 'v1'], 'v1': 'v1'})
  #second abstraction "more general" than first
  print_and_assert_equal(match([2,2,'v1',1], ['v0', 1], {'v0': [[1], 'v1'], 'v1': 'v1'}))
  #simpler case variables do not overlap, match exists
  print_and_assert_equal(match([1,'v0'], ['v1', 1]), {'v0': 1, 'v1': 1})
#+end_src

#+RESULTS:

Another way to think of the generalization of match is as a way to make the abstractions more concrete or to add structure to each argument based on the structure of the other argument.  If we view abstractions as descriptions of sets, the new match tries to find the intersection (or some subset) of the sets described by the passed in argument.  In this view abstraction is then finding a description of a set that contains the items being abstracted over.  Inference is the process of trying to specify an element of a previously seen set based on some constraints.

Let's start the generalized definition of match.  One difference is we will need to do some preprocessing to ensure unique variable names between the two data being processed.  The main difference is we will need to be "duplicate" how we process the partial_data for the second argument.  

#+name: structure_for_match
#+begin_src python :noweb yes :tangle yes
  def match(data1, data2):
      if is_variable(data1):
          return {data1: data2}
      elif is_variable(data2):
          return {data2: data1}
      elif is_symbol(data1) or is_symbol(data2):
          if data1 == data2:
              return {}
          else:
              return None
      else:
          assert(isinstance(data1, list) and isinstance(data2, list))
          <<match_list_case>>
#+end_src

In the match list case we will need to be able to "fast-forward" the iteration through both data1 and data2 (as opposed to just data2 in the previous version of match) since there might be variables in data2.  

#+name: match_list_case
#+begin_src python :noweb yes
  #using a dictionary b/c has reference passed into functions allowing changes to be permanent after
  #exiting the function
  matches = []
  data1_position = 0
  data2_position = 0
  #iterate until one of the data has been examined completely
  while data1_position < len(data1) and data2_position < len(data2):
      #in the case where you can match a variable to a sublist of the other data
      #fast-forward the position in the other data
      if is_variable(data1[data1_position]) and data1_position < len(data1)-1:
          data2_position, matches = bind_variable(data1_position, data1,
                                                  data2_position, data2,
                                                  matches)
          data1_position += 2
      elif is_variable(data1[data1_position]):
          #the last element of data1 is a variable so bind it to the rest of data2
          matches.append(match(data1[data1_position], data2[data2_position:]))
          data1_position += 1
          data2_position = len(data2)
      elif is_variable(data2[data2_position]) and data2_position < len(data2)-1:
          data1_position, matches = bind_variable(data2_position, data2,
                                                  data1_position, data1,
                                                  matches)
          data2_position += 2
      elif is_variable(data2[data2_position]):
          #the last element of data2 is a variable so bind it to the rest of data1
          matches.append(match(data1[data1_position:], data2[data2_position]))
          data2_position += 1
          data1_position = len(data1)
      else:
          matches.append(match(data1[data1_position],
                                        data2[data2_position]))
          data1_position += 1
          data2_position += 1
  if data1_position < len(data1) or data2_position < len(data2):
      return None
  <<match_list_case_combine_matches>>
  
#+end_src

#+name: bind_variable
#+begin_src python :noweb yes :tangle yes
  def bind_variable(variable_position, variable_data, other_position, other_data, matches):
      """
      Function for matching the variable in one data to some subset of the other data
      Fast forwards other_position and changes matches
      """
  
      <<find_matching_positions>>
  
      variable = variable_data[variable_position]
      element_after_variable = variable_data[variable_position+1]
      #find the possible matching positions in nonvariable data and possible bindings if
      #the matching position corresponds to a variable
      matching_positions = find_matching_positions()
      #randomly choose a match if one exists
      if len(matching_positions) > 0:
          chosen_match_position = random.choice(matching_positions.keys())
          #binding for elements right after what is bound to variable_position variable
          matches.append(matching_positions[chosen_match_position])
          #binding for variable_position variable
          matches.append({variable: other_data[other_position:chosen_match_position]})
          other_position = chosen_match_position+1
      else:
          matches.append(None)
      return other_position, matches
#+end_src

Before find_matching_positions was a matter of searching through the non-variable data for elements that matched the element after the variable.  Finding match positions in this version can differ slightly because variables in the "non-variable" (other_data) can also be potential matches.  How should we treat variables in other_data?  Let's example the case of [2,2,'v1',1] and  ['v0', 1].  If 'v0' is the variable then we will try to find a match for 1 in [2,2,'v1',1].  When we encounter 'v1' is it a possible match?  If we did try and match it then the overall match would be None since the last 1 in [2,2,'v1',1] would not match to anything in ['v0', 1].  Can we use the number of non-variables in either data as a constraint?  The possible cases are: 
1) number of non-variables in variable_data exceeds number of non-variables after the binding in other_data 

or 

2) number of non-variables in variable_data is less than number of non-variables after the binding

We can further subdivide cases by looking at whether the remaining data has variables.

1.1 if there are no variables in other_data after the binding then there cannot be a match in this case since there would be some non-variable in variable-data that does not match to anything

1.2 if there are variables in other_data then one cannot use the same argument in 1.1 since a variable in other_data may bind to more than one non-variable in the remaining non-variable data.

Rather than finish this analysis let us choose a simpler algorithm that may make "mistakes" in favor of increasing the probability match will find a valid match if it exists.  Possible matching positions are either non-variable elements that match or variables.

#+name: find_matching_positions
#+begin_src python :noweb yes
  def find_matching_positions():
      element_after_variable = variable_data[variable_position+1]
      possible_positions = {}
      for new_position, other_element in enumerate(other_data[other_position:], other_position):
          #we record the matches in case other_element is a variable, if the corresponding
          #position is chosen we can add the match to matches
          possible_match = match(element_after_variable, other_element)
          if possible_match is not None:
              possible_positions[new_position] = possible_match
      return possible_positions
#+end_src

Now that we've rewritten match let's see how it does on the inference problems the first approximation couldn't solve.

The first up is classify_problem0 with the abstraction we got from the first two examples of the pattern0.
#+begin_src python :noweb yes :tangle second_approximation.py
  print match(classify_problem0.partial_data, [[1, 'v1'], 0])
#+end_src

#+RESULTS:
: {'v0': [0], 'v1': [1, 1, 1]}

******* Adding abstraction to the agent functions
In the last section we manually performed the abstraction step as we added data to the agent.  Let's adjust the add function for memory to do this automatically so it's easier to test the new approach for solving inference problems.

In addition to storing new data the agent gets we'll also want to create and store abstractions; we'll need to make a few design decisions for the new implementation.  For this version when we get new data we'll only create abstractions with "non-abstract" data.  The motivation is to  more easily identify the need for more complicated abstraction by trying to solve a problem where abstracting between concrete and abstract data is needed.  To help accomplish this we'll store abstractions separately from the non-abstract data and perhaps as we introduce higher order abstractions each level of abstraction will be kept separately although we'll have to see what benefit there might be to this strategy [DD].

#+name: memory_deprecated
#+begin_src python :noweb yes :tangle no
    class Memory:
        def __init__(self):
            self.concrete_data = []
            self.abstract_data = []
    
        def add(self, new_data):
            for prior_data in self.concrete_data:
                self.abstract_data.append(abstract(prior_data, new_data))
            self.concrete_data.append(new_data)
            
    
        def dump(self):
            return self.concrete_data+self.abstract_data
    
#+end_src

Let's test out the new structure for memory by adding data.

#+begin_src python :noweb yes :tangle test_memory.py
  from abstraction import *
  from utility import *
  
  <<binary_data>>
  
  agent = Agent()
  agent.memory.add(pattern0[0])
  print_and_assert_equal(agent.memory.dump(), [pattern0[0]])
  agent.memory.add(pattern0[1])
  print_and_assert_equal(agent.memory.dump(), [[[1], 0], [[1, 1], 0], [[1, 'v0'], 0]])
  agent.memory.add(pattern1[0])
  print_and_assert_equal(agent.memory.dump(), [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4']])
  agent.memory.add(pattern1[1])
  print_and_assert_equal(agent.memory.dump(), [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1]])
#+end_src

Now that we've changed the way data is stored in memory we must also adjust how the agent solves inference problems.  The basic algorithm will be to try to match the problem to concrete data and if there is no match from that then try to match it to an abstraction [DD].

#+name: agent_solve_deprecated
#+begin_src python :noweb yes
  solutions = [match(inference_problem.partial_data, prior_data)
                 for prior_data in self.memory.concrete_data]
  
  solutions = [solution for solution in solutions if solution != None]
  
  if solutions == []:
      solutions = [match(inference_problem.partial_data, prior_data)
                   for prior_data in self.memory.abstract_data]
  
      solutions = [solution for solution in solutions if solution != None]
  
  if solutions == []:
      return None
  else:
      return random.choice(solutions)
#+end_src

Let's test this new agent code by trying to solve some inference problems.

******** Solving classify_problem0

Let's redo the manual steps from last time and verify we get the same results, we can reuse most of second_approximation.py, but now solve should be able to find a solution after getting pattern0[0] and pattern0[1] so we don't need to manually match partial data to [[1,'v0'], 0].


#+begin_src python :noweb yes :results output :tangle second_approximation2.py
  from abstraction import *
  from utility import *
  
  <<binary_data>>
  <<binary_data_classification_problems>>
  <<additional_binary_classification_problems>>
  <<additional_binary_completion_problems>>
  
  agent = Agent()
  print 'classify_problem0 %s' % classify_problem0.partial_data
  print_and_assert_equal(agent.solve(classify_problem0), None)
  agent.memory.add(pattern0[0])
  print 'agent memory %s' % agent.memory.dump()
  print_and_assert_equal(agent.solve(classify_problem0), None)
  agent.memory.add(pattern0[1])
  print 'agent memory %s' % agent.memory.dump()
  print agent.solve(classify_problem0)
  
#+end_src

#+RESULTS:
: classify_problem0 [[1, 1, 1, 1], 'x0']
: None
: agent memory [[[1], 0]]
: None
: agent memory [[[1], 0], [[1, 1], 0], [[1, 'v0'], 0]]
: {'v0': [1, 1, 1], 'x0': [0]}

While the agent came up with the correct solution in this case, we should address an issue related to naming of variables.  The problem is in the inference problem 'v0' was the name of the variable we're trying to solve for while 'v0' was also the name of the variable in our abstraction learned from data.  Perhaps the simplest solution for now would be to change the variable name/symbol in the inference problem [DD].  This will make it easier to determine what the solution is when looking at the bindings returned by match since it distinguishes the inference problem variables from the abstraction variables.  In the future we might think about how to maintain separate environments in match and returning two sets of bindings.

#+name: binary_data_classification_problems
#+begin_src python :noweb yes
  classify_problem0 = InferenceProblem(['x0'],[[1,1,1,1], 'x0'])  
  classify_problem1 = InferenceProblem(['x0'], [[1,1,1,0,0,0], 'x0'])
#+end_src

We'll also need to adjust our way of checking whether something is a variable

#+name: is_variable
#+begin_src python :noweb yes :tangle yes
  def is_variable(thing):
      return isinstance(thing, str) and (thing[0] == 'v' or thing[0] == 'x') \
        and thing[1:].isdigit()
#+end_src

We can see the results in the evaluation of second_approximation2.py above.  

******** Solving classify_problem1
classify_problem1 does not require induction to solve after seeing pattern1[1]
******** Solving classify_problem2
#+begin_src python :noweb yes :tangle second_approximation2.py
  print 'classify_problem2 %s' % classify_problem2.partial_data
  print agent.solve(classify_problem2)
#+end_src

#+RESULTS:
: classify_problem2 [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'x0']
: {'v0': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'x0': [0]}

******** Solving classify_problem3
Let's continue to check how our new use of abstraction by solving classify_problem3.  So far we've added pattern0[0] and pattern0[1] to memory.  Let's try to 

#+begin_src python :noweb yes :tangle second_approximation2.py
  print 'classify_problem3 %s' % classify_problem3.partial_data
  print agent.solve(classify_problem3)
#+end_src

#+RESULTS:
: agent memory [[[1], 0], [[1, 1], 0], [[1, 'v0'], 0]]
: classify_problem3 [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x0']
: {'v0': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x0': [0]}

This example brings up the issue of how we deal with error.  Here we have an abstraction that produces an incorrect inference, how should the agent adjust once this is realized?  How should the agent receive feedback about the error? [OP]  One option is to not do anything explicit in changing the inference algorithm, but just add the correctly completed data from the inference problem to memory [DD].  This will mean the next time the problem is encountered it will be answered correctly.  Other possibilities for what to do when an error occurs may arise as the algorithm for selecting which abstractions to use when solving an inference problem become more sophisticated.

#+begin_src python :noweb yes :tangle no
  agent.memory.add([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1])
  print 'agent memory %s' % agent.memory.dump()
  print agent.solve(classify_problem3)
#+end_src

#+RESULTS:
: agent memory [[[1], 0], [[1, 1], 0], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4']]
: {'x0': [1]}

Let's backtrack for a second and see if we can solve classify_problem1 when we have more prior data for pattern1.

#+begin_src python :noweb yes :tangle second_approximation2.py
  agent.memory.add(pattern1[0])
  print 'agent memory %s' % agent.memory.dump()
#+end_src

#+RESULTS:
: agent memory [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4']]


#+begin_src python :noweb yes :tangle second_approximation2.py
  agent.memory.add(pattern1[1])
  print 'agent memory %s' % agent.memory.dump()
#+end_src

#+RESULTS:
: agent memory [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1]]

Now we have an abstraction that can be used to solve classify_problem3, namely [[1, 1, 'v9', 0, 0, 'v10'], 1].

#+begin_src python :noweb yes :tangle second_approximation2.py
  print match(classify_problem3.partial_data, [[1, 1, 'v9', 0, 0, 'v10'], 1])
#+end_src

Unfortunately this is a case where our simplistic match of variables can err (e.g. v9 might try to bind to [1], [1,0], [1,0,0], or [1,0,0,0], but in the case it finds the correct binding we get

#+RESULTS:
: {'v9': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'v10': [0, 0, 0, 0], 'x0': [1]}

Note that we lack constraints on the number of 1s and 0s in our abstraction.


#+begin_src python :noweb yes :tangle second_approximation2.py

  print agent.solve(classify_problem3)
#+end_src

#+RESULTS:
: {'v9': [1, 1, 1, 1, 1, 1, 1, 1], 'v10': [0, 0, 0, 0, 0, 0, 0, 0], 'x0': [1]}
We need a more sophisticated way of choosing between abstractions.

******** Solving completion_problem2
#+begin_src python :noweb yes :tangle second_approximation2.py
  print 'completion_problem2 %s' % completion_problem2.partial_data
  print agent.solve(completion_problem2)
  assert agent.solve(completion_problem2) != None
#+end_src

#+RESULTS:

******** An aside on time-based execution
One way to approach the problem of not exhaustively searching all possibilities when performing various operations like match is to run the function for a set amount of time and either return the first answer or the best.

    solutions = [match(inference_problem.partial_data, prior_data)
                   for prior_data in self.memory.concrete_data]
    
    solutions = [solution for solution in solutions if solution != None]
    
    if solutions == []:
        solutions = [match(inference_problem.partial_data, prior_data)
                     for prior_data in self.memory.abstract_data]
    
        solutions = [solution for solution in solutions if solution != None]
    
    if solutions == []:
        return None
    else:
        return random.choice(solutions)
  #+name: time_agent
#+begin_src python :noweb yes :tangle yes
  from time import time
  class Agent:
      def __init__(self):
          self.memory = Memory()
  
      def solve(self, inference_problem):
          <<agent_solve>>
  
      def solve_within(self, inference_problem, duration=None):
          start = time()
          while time()-start < duration:
              solution = self.solve(inference_problem)
              if solution is not None:
                  break
          return solution
#+end_src

Let's see if this helps in the case where match can fail like in the match_fail code block.

#+name: match_fail_solved
#+begin_src python :results output :noweb yes
  from abstraction import *
  agent = Agent()
  agent.memory.add([1,0,1,0,1,0,0])
  print agent.memory.dump()
  problem = InferenceProblem(['x0'], ['x0', 0])
  print [agent.solve(problem) for i in range(10)]
  print [agent.solve_within(problem, 3000) for i in range(10)]
#+end_src

#+RESULTS: match_fail_solved
: [[1, 0, 1, 0, 1, 0, 0]]
: [{'x0': [1, 0, 1, 0, 1, 0]}, None, None, None, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, None, None, None, None]
: [{'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}, {'x0': [1, 0, 1, 0, 1, 0]}]

We see here that solve_within is useful for getting the correct answer every time.
******** A more abstract pattern0
In this section we'll begin to try and learn a more abstract version of pattern0 that can be described as a sequence of the same character.  We'll start by trying to solve classify_problem4 with the data we have then add additional prior_data of pattern0 to see if any useful abstraction comes out of it.  Solving these problems may require changes to the way abstraction work.
********* solving classify_problem4
Let's attempt to solve classify_problem4 with the data we already have in memory.

#+begin_src python :noweb yes :tangle second_approximation2.py
  <<surface_level_abstraction_problems>>

  print 'current memory: %s' % agent.memory.dump()
  print 'classify_problem4 %s' % classify_problem4.partial_data
  print agent.solve(classify_problem4)
#+end_src

#+RESULTS:
: current memory: [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1]]
: classify_problem4 [[8, 8, 8, 8, 8], 'v0']
: None

Right now the current "understanding" of pattern0 is at best decribed as 1 followed by something captured by [[1, 'v0'], 0].  Let's add some of the additional data about pattern0 that does not fit this mold.

#+begin_src python :noweb yes :tangle second_approximation2.py
  <<additional_prior_data>>
  
  agent.memory.add(more_pattern0[0])
  print 'agent memory: %s' % agent.memory.dump()
#+end_src

#+RESULTS:
: agent memory: [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[2, 2], 0], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1], [['v11', 'v12'], 0], [['v13', 'v14'], 0], [['v15', 'v16', 'v17'], 'v18'], [['v19', 'v20', 'v21'], 'v22']]

Now if we try to solve classify_problem4 we get the following

#+begin_src python :noweb yes :tangle second_approximation2.py
  print 'classify_problem4 %s' % classify_problem4.partial_data
  print agent.solve(classify_problem4)
#+end_src

which if you run enough times you get
#+RESULTS:
: {'v12': 8, 'x0': [0], 'v11': [8, 8, 8, 8]}

There are a few things to note.  The first is the abstractions created from pattern0[0] e.g. [['v11', 'v12'], 0] and [['v19', 'v20', 'v21'], 'v22'].  One thing we've ignored until now is choosing between applicable abstractions.  In this case we had to run solve several times before [['v11', 'v12'], 0] was chosen.  One question is how should the choice be made of what abstraction to use [OP] (open problem)?  Another issue is these abstractions don't really capture the structure of pattern0; the reason being they are too general (and in fact this has been a problem all along e.g. [[1, 'v0'], 0], which was used to solve earlier inference problems of recognizing pattern0 could also be used for pattern1.

An example of this is if we try to solve classify_problem3

#+begin_src python :noweb yes :tangle second_approximation2.py
  print classify_problem3.partial_data
  print agent.solve(classify_problem3)
#+end_src

If we run it enough times we'll get

#+RESULTS:
: {'v0': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x0': [0]}

We'll explore the question of what kind of abstraction is needed to distinguish pattern0 from pattern1 as well as how to generate such an abstraction from data.

********* Abstractions for distinguishing between pattern0 and pattern1
Pattern0 can intuitively be described as a repeated sequence of the same character.  How could one capture this idea in terms of abstractions?  If we add more data from pattern0 what happens?

#+begin_src python :noweb yes :tangle second_approximation2.py
  print agent.memory.dump()
  print 'adding more pattern0: %s' % more_pattern0[1]
  agent.memory.add(more_pattern0[1])
  print agent.memory.dump()
#+end_src

#+RESULTS:
: [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[2, 2], 0], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1], [['v11', 'v12'], 0], [['v13', 'v14'], 0], [['v15', 'v16', 'v17'], 'v18'], [['v19', 'v20', 'v21'], 'v22']]
: adding more pattern0: [[3, 3, 3, 3, 3], 0]
: [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[2, 2], 0], [[3, 3, 3, 3, 3], 0], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1], [['v11', 'v12'], 0], [['v13', 'v14'], 0], [['v15', 'v16', 'v17'], 'v18'], [['v19', 'v20', 'v21'], 'v22'], [['v23', 'v24'], 0], [['v25', 'v26', 'v27'], 0], [['v28', 'v29', 'v30', 'v31', 'v32'], 'v33'], [['v34', 'v35', 'v36', 'v37', 'v38', 'v39'], 'v40'], [['v41', 'v42', 'v43'], 0]]

#+begin_src python :noweb yes :tangle second_approximation2.py
  print agent.memory.dump()
  print 'adding more pattern0: %s' % more_pattern0[2]
  agent.memory.add(more_pattern0[2])
  print agent.memory.dump()
#+end_src

#+RESULTS:
: adding more pattern0: [[5, 5, 5, 5, 5, 5, 5], 0]
: [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[2, 2], 0], [[3, 3, 3, 3, 3], 0], [[5, 5, 5, 5, 5, 5, 5], 0], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1], [['v11', 'v12'], 0], [['v13', 'v14'], 0], [['v15', 'v16', 'v17'], 'v18'], [['v19', 'v20', 'v21'], 'v22'], [['v23', 'v24'], 0], [['v25', 'v26', 'v27'], 0], [['v28', 'v29', 'v30', 'v31', 'v32'], 'v33'], [['v34', 'v35', 'v36', 'v37', 'v38', 'v39'], 'v40'], [['v41', 'v42', 'v43'], 0], [['v44', 'v45'], 0], [['v46', 'v47', 'v48'], 0], [['v49', 'v50', 'v51', 'v52', 'v53'], 'v54'], [['v55', 'v56', 'v57', 'v58', 'v59', 'v60', 'v61'], 'v62'], [['v63', 'v64', 'v65'], 0], [['v66', 'v67', 'v68', 'v69', 'v70', 'v71'], 0]]

Clearly the current mechanisms for abstracting and matching patterns will not capture the main features of pattern0 that distinguish it from pattern1.  One direction is to introduce naming of abstractions and think of how to get the system to create recursive functions.  Another direction (which may turn out to be similar), which solves a more general problem is to better define how an input can be decomposed and reformulated in terms of its parts/multiple abstractions.  Recursion can be understood as a special case where the multiple parts are the same abstraction.  Another problem that needs to be addressed is even if we can formulate the abstractions required for a given pattern, how do we select the correct ones when matching? [OP] In either case we will need to modify how abstraction and solving work so let us start a new approximation.  


*** Third Approximation
The previous approximation allowed us to produce solutions to basic inference problems, but often times these solutions could be incorrect due to the random way abstractions were selected for matching.  Another reason was the limited patterns that could be expressed through the basic abstraction mechanism.  The question now is whether we should guide this next approximation by trying to enrich the abstraction process or by trying to make the matching process more selective.  Each will probably impact the other.  Let's work on match so that we can consistently solve the simple problems and then add complexity to solve harder problems.

The issue we face now is that we have no criteria for choosing between different abstractions that exist in memory.  An example of why this is a problem can be seen in the following set up.  Suppose we introduce enough data from pattern0 to form an abstraction for it...

---aside---
There seems to be some common code we will be including each time so let's factor that out into its own code block

#+name: imports_data_and_problems
#+begin_src python :noweb yes
  from abstraction import *
  from utility import *
  
  <<binary_data>>
  <<additional_prior_data>>
  <<binary_data_classification_problems>>
  <<additional_binary_classification_problems>>
  <<additional_binary_completion_problems>>
#+end_src
-----------

#+begin_src python :noweb yes :results output :tangle third_approximation_problem.py
  <<imports_data_and_problems>>
  
  agent = Agent()
  
  agent.memory.add(pattern0[0])
  print_and_assert('added pattern0[0]', equal, agent.memory.dump(), [pattern0[0]])
  agent.memory.add(pattern0[1])
  print_and_assert('added pattern0[1]', equal, agent.memory.dump(), [[[1], 0], [[1, 1], 0], [[1, 'v0'], 0]])
  
#+end_src

#+RESULTS:
: added pattern0[0] => [[[1], 0]]
: added pattern0[1] => [[[1], 0], [[1, 1], 0], [[1, 'v0'], 0]]

and we also introduce enough data from pattern1 to form an abstraction of it...

 
#+begin_src python :noweb yes :tangle third_approximation_problem.py
  agent.memory.add(pattern1[0])
  print_and_assert('added pattern1[0]', equal, agent.memory.dump(), [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4']])
  agent.memory.add(pattern1[1])
  print_and_assert('added pattern1[1]', equal, agent.memory.dump(), [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1]])
  
#+end_src

#+RESULTS:
: added pattern1[0] => [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4']]
: added pattern1[1] => [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1], [[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1]]

Now if we try to solve a problem of recognizing pattern1 we may get either pattern0 or pattern1.


#+begin_src python :noweb yes :tangle third_approximation_problem.py
  #classify_problem3 = InferenceProblem(['x0'], [[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0], 'x0'])
  print [agent.solve(classify_problem3) for i in range(10)]
pr
#+end_src

#+RESULTS:
: [{'x0': ['v6'], 'v5': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'x0': ['v8'], 'v7': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'v1': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x0': ['v2']}, {'v9': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], 'v10': [0, 0, 0, 0, 0], 'x0': [1]}, {'v9': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], 'v10': [0, 0, 0, 0], 'x0': [1]}, {'x0': ['v8'], 'v7': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'v0': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x0': [0]}, {'x0': ['v4'], 'v3': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'x0': ['v6'], 'v5': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'x0': ['v4'], 'v3': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]

We see match can result in many possible solutions.  Let's explore some possible principles for selecting different abstractions when matching.

**** More sophisticated matching
One solution for the problem demonstrated above is to choose more specific matches i.e. choosing abstractions where more of the data matches.  In the case above [[1, 'v0'], 0] matches 1 and 0 in [[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0], 'x0'] whereas [[1, 1, 'v9', 0, 0, 'v10'], 1] matches 1,1 and 0,0 and 1.  Is it always the case that more specific matches will produce better inferences [OP]?  This seems ok, but intuitively it seems like we should explore how match can be improved by hierarchical organization of abstractions.  By exploring hierarchical search for match we can also implement finding the most specific match as a criteria.

Let's revisit the idea of an abstraction representing a set.  Given a bunch of sets (abstractions) how might we decide which is the best to try and match to?  One idea would be to say if a lot of previous data was found to be in a set (matched an abstraction) then it is likely new data would also be in that set.  We can also think of the sets hierarchically with more specific abstractions being sets contained more general abstractions.  This lends itself to hierarchical search when matching where we first look at the largest sets then narrow down.  

Let's try and combine these two ideas in our new version of match.  Let's first restructure abstractions in memory so that hierarchical searching is easy, then keep track of how often data fit into a particular set (i.e. match an abstraction) and use these counts to guide search.  Lastly it'd be nice to develop some more rigorous theoretical arguments motivating  two design decisions.

***** Hierarchical search through abstractions
Two aspects we'll need to change in order to have hierarchical search are 
1. the way abstractions are organized in memory
2. the algorithm for searching through memory

****** Organizing abstractions
Inutuitively it seems like we want the "most abstract" abstractions at the top of the hierarchy.  One reason is these are the largest sets i.e. they're the most general abstractions and thus will match against the most things.  What makes one abstraction more general than another?  If the set the abstraction represents contains the set the other abstraction represents then the first abstraction is more general.  Some questions arise if we go down this path.  What about the case of abstractions of abstractions?  What about sets that intersect, but are not completely contained within another?  What about recursive abstractions?

Let's look at the set of  abstractions we get from our exisitng method of abstraction and see how if changes in organization can help solve the problem of choosing the most appropriate abstraction when solving an inference problem.

******* Abstractions for [[[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1]]
Given the first two instances of pattern0 and pattern1 we get the following abstractions

[[[1, 'v0'], 0], [[1, 'v1'], 'v2'], [[1, 1, 'v3'], 'v4'], [[1, 'v5'], 'v6'], [[1, 1, 'v7'], 'v8'], [[1, 1, 'v9', 0, 0, 'v10'], 1]]


Intuitively it seems like the most important abstractions are [[1, 'v0'], 0] and [[1, 1, 'v9', 0, 0, 'v10'], 1] in the sense that these can be used to infer the solution to the classfiication problems (since they have a non-variable in the class position).  This brings up a point about the relevance of abstractions base on the problem, is there one "best" hierarchy for the abstractions given how different the inference problems can be?

If we try to group the abstractions based on the set interpretation we get  
[[1, 'v1'], 'v2'] === [[1, 'v5'], 'v6'] at the top

[[1, 1, 'v7'], 'v8'] ==== [[1, 1, 'v3'], 'v4'] contained in [[1, 'v1'], 'v2']

[[1, 'v0'], 0] contained in [[1, 'v1'], 'v2'] and intersecting with [[1, 1, 'v7'], 'v8']

and

[[1, 1, 'v9', 0, 0, 'v10'], 1] contained in [[1, 1, 'v7'], 'v8']

Two questions that arise are 1) does this organization help solve inference problems? 2) if it does how should the organization be implemented?

******** Example of Using hierarchical organization to solve inference problems
Let's look at the original problem that motivated this exploration of hierarchical organization then examine other inference problems

********* classify_problem3
We were trying to solve classify_problem3 ([[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0], 'x0']) given data [[1], 0], [[1, 1], 0], [[1, 1, 0, 0], 1], [[1, 1, 1, 0, 0, 0], 1].

Given the above organization, one way to try and match is to start from the top and move downward.

Matching [[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0], 'x0'] with [[1, 'v1'], 'v2'] returns


#+begin_src python :noweb yes
  from abstraction import match
  import pdb
  print match([[1, 'v1'], 'v2'], [[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0], 'x0'])
#+end_src

#+RESULTS:
: {'v1': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'v2': ['x0']}

Now we can try to match against abstractions lower in the hierarchy until we get to the lowest level, which would be [[1, 1, 'v9', 0, 0, 'v10'], 1].  Why matches against the lowest level be the best matches for inference [OP]?

********* classify_problem2
For classify_problem2 ([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'x0']) we would first match to [[1, 'v1'], 'v2'], then [[1, 1, 'v7'], 'v8'] and [[1, 'v0'], 0].  [[1, 1, 'v9', 0, 0, 'v10'], 1] does not match so we have to choose between [[1, 1, 'v7'], 'v8'] and [[1, 'v0'], 0].  In this case it seems [[1, 'v0'], 0] is a less abstract match (0 vs. 'v8').
********* Additional sophistication
The previous descriptions are pretty basic ways of organizing and using hierarchical structure. It's easy to see there is quite a bit of redundancy in the abstractions and in another approximation we'll explore how to create more compact representations and how to use them in matching [OP].
******* Implementation of hierarchical organization and solving algorithm
In order to support the more complicated organization of abstractions we'll need to modify the data structure for memory.  We'll also need to modify the solve function to take advantage of the new structure.
******** Hierarchical Solve
Let's first look at how solve will work to determine the interface for the hierarchical memory organization.  The general algorithm will be to start at the highest level abstractions and traverse down for each finding the lowest level match then take the "least abstract" match.  

#+name: agent_solve
#+begin_src python :noweb yes :tangle yes
  <<find_lowest_match>>
  <<get_least_abstract_match>>
  possible_solutions = []
  for top_abstraction in self.memory.top_abstractions:
      possible_solutions.append(find_lowest_match(top_abstraction, inference_problem.partial_data))
  
  return get_least_abstract_match(possible_solutions)
#+end_src

Traversing down a hierarchy is a recursive process of trying to match the current abstraction and if there is no match then returning the parent, if there is a match then seeing if there are any children abstractions and if so trying to match against them. 
#+name: find_lowest_match

#+begin_src python :noweb yes
  def find_lowest_match(top_abstraction, partial_data):
      leaves = []
      def traverse(abstraction_set, depth):
          match = abstraction.match(abstraction_set, partial_data)
          if match == None:
              return None
          elif abstraction_set.get_children() == []:
              leaves.append((match, depth))
              return True
          else:
              child_matches = [traverse(child_abstraction_set, depth+1) for child_abstraction_set
                               in abstraction_set.get_children()]
              if not any(child_matches):
                  leaves.append(match, depth)
                  return True
              else:
                  return None
  
      traverse(top_abstraction, 0)
      
#+end_src

******** Memory structure
The basic idea of the hierarchical organization We can think of every abstraction as a set so let's create a data structure that associates a set with an abstraction.

#+name: abstraction_set
#+begin_src python :noweb yes :tangle yes
  class AbstractionSet:
      def __init__(self, abstraction):
          #set of AbstractionSets
          self.set = set([])
          self.abstraction = abstraction
  
      def contains(self, abstraction):
          <<abstraction_set_containment>>
#+end_src



#+name: memory
#+begin_src python :noweb yes
  class Memory:
      def __init__(self):
          self
#+end_src


***** Counting matches
***** Theoretical justifications




